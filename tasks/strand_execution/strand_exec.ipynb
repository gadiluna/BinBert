{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2e1abc9",
   "metadata": {
    "executionInfo": {
     "elapsed": 4864,
     "status": "ok",
     "timestamp": 1647423034663,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "b0c12784"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "\n",
    "# Transformer tokenizer imports\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Transformers data collator\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "# Transformers Bert model\n",
    "from transformers import AutoModelForSequenceClassification, BertForPreTraining, Trainer, TrainingArguments, EarlyStoppingCallback, BertConfig\n",
    "\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd20ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU settings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0,1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d28ea2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647423034664,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "bf324773"
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "    # load tokenizer from dict\n",
    "    tokenizer =  BertTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2da2410",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647423034664,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "4be39a33"
   },
   "outputs": [],
   "source": [
    "class StrandExecDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, tokenizer):\n",
    "        \n",
    "        self.data_store = []\n",
    "        df = pd.read_csv(dataset_path, sep=\"\\t\").fillna('')\n",
    "        self.samples = df[[\"ot_instructions\", \"ot_inputs\", \"ot_concrete_expr\"]]\n",
    "        self.tokenizer = tokenizer\n",
    "            \n",
    "        self.__init_structures()\n",
    "         \n",
    "    def __init_structures(self):\n",
    "        \n",
    "        for x, inputs, expr in tqdm(self.samples.values):\n",
    "            inputs = json.loads(inputs.replace(\"\\'\", \"\\\"\"))\n",
    "            expr = json.loads(expr.replace(\"\\'\", \"\\\"\"))\n",
    "            y = \"\"\n",
    "            for k in inputs:\n",
    "                y += f\" {k} = {inputs[k]}\"\n",
    "            for k in expr:\n",
    "                y += f\" {k}\"\n",
    "                label = expr[k]\n",
    "            example = self.tokenizer(text=x, text_pair=y, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "            example[\"label\"] = label\n",
    "            self.data_store.append(example)\n",
    "            \n",
    "        random.shuffle(self.data_store)\n",
    "                \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_store)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.data_store[idx]\n",
    "    \n",
    "    def save_to_file(self,save_file):\n",
    "        torch.save(self.data_store, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee94364",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6427880e",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647423034663,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "6341a645"
   },
   "outputs": [],
   "source": [
    "from_scratch = True\n",
    "\n",
    "#if from_scratch:\n",
    "# LEARNING_RATE = 0.0001\n",
    "#else:\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 32\n",
    "DATA_LOADER_NUM_WORKERS = 4\n",
    "PATIENCE = 3\n",
    "\n",
    "#models\n",
    "BXSMAL=\"bert_xsmall\"\n",
    "BSMAL=\"bert_small\"\n",
    "BNORM=\"bert_normal\"\n",
    "BLARG=\"bert_larg\"\n",
    "\n",
    "MODEL=BNORM\n",
    "if MODEL == BXSMAL:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 128\n",
    "    INTERMEDIATE_SIZE = 1024\n",
    "    NUM_ATTENTION_HEADS = 8\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "\n",
    "if MODEL == BSMAL:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 512\n",
    "    INTERMEDIATE_SIZE = 2048\n",
    "    NUM_ATTENTION_HEADS = 8\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "    \n",
    "if MODEL == BNORM:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 768\n",
    "    INTERMEDIATE_SIZE = 3072\n",
    "    NUM_ATTENTION_HEADS = 12\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2    \n",
    "    \n",
    "if MODEL == BLARG:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 1024\n",
    "    INTERMEDIATE_SIZE = 4096\n",
    "    NUM_ATTENTION_HEADS = 16\n",
    "    NUM_HIDDEN_LAYERS = 24\n",
    "    TYPE_VOCAB_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c9b2bec",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1647423034663,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "a92cd8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../../\"\n",
    "prt_model = os.path.join(base_path, \"models\", \"pretraining_model\", \"checkpoint-67246\")\n",
    "train_path = os.path.join(base_path, \"dataset\", \"finetuning_dataset\", \"strand_execution\", \"train_concrete_execution.csv\")\n",
    "val_path = os.path.join(base_path, \"dataset\", \"finetuning_dataset\", \"strand_execution\", \"val_concrete_execution.csv\")\n",
    "tokenizer_path = os.path.join(base_path, \"tokenizer\")\n",
    "\n",
    "model_name =  f\"BinBert_strand_execution\"\n",
    "output_dir = model_path = os.path.join(base_path, \"models\", \"finetuned_models\", \"strand_execution\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a054e181",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166095,
     "status": "ok",
     "timestamp": 1647423200755,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "102b9759",
    "outputId": "09219005-711d-4dcd-8205-3e0bb68dcb86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:05<00:00, 7465.08it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 8314.18it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "train_dataset = StrandExecDataset(train_path, tokenizer)\n",
    "val_dataset = StrandExecDataset(val_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0640e230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200}\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "for elem in train_dataset.data_store:\n",
    "    labels.add(elem[\"label\"])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53634ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2079,
     "status": "ok",
     "timestamp": 1647423202821,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "312c3d3c",
    "outputId": "1bd00a61-fa01-47a9-b5f2-36f52b32b4fb"
   },
   "outputs": [],
   "source": [
    "if not from_scratch:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(prt_model, num_labels=len(labels))\n",
    "else:\n",
    "    config = BertConfig(\n",
    "                vocab_size = len(tokenizer.vocab),\n",
    "                max_position_embeddings = MAX_POSITION_EMBEDDINGS,\n",
    "                hidden_size = HIDDEN_SIZE,\n",
    "                intermediate_size = INTERMEDIATE_SIZE,\n",
    "                num_attention_heads = NUM_ATTENTION_HEADS,\n",
    "                num_hidden_layers = NUM_HIDDEN_LAYERS,\n",
    "                type_vocab_size = TYPE_VOCAB_SIZE\n",
    "    )\n",
    "    config.num_labels = len(labels)\n",
    "    model = AutoModelForSequenceClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79275c01",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647423202823,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "44388bf6"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "                    output_dir = output_dir,\n",
    "                    overwrite_output_dir = True,\n",
    "                    num_train_epochs = NUM_TRAIN_EPOCHS,\n",
    "                    learning_rate = LEARNING_RATE,\n",
    "                    per_device_train_batch_size = PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                    save_strategy = 'epoch',\n",
    "                    save_total_limit = 1,\n",
    "                    logging_strategy = 'epoch',\n",
    "                    # prediction_loss_only = True,\n",
    "                    # fp16=True,\n",
    "                    load_best_model_at_end = True,\n",
    "                    do_eval = True,\n",
    "                    evaluation_strategy = 'epoch',\n",
    "                    metric_for_best_model = 'eval_accuracy',\n",
    "                    per_device_eval_batch_size = PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                    dataloader_num_workers = DATA_LOADER_NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c997b3f7",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647423202824,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "9ddd4097"
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a188ded",
   "metadata": {
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1647423203861,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "bfc4570b"
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65cefd49",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1647423203862,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "faeacf91"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce05c15d",
   "metadata": {
    "executionInfo": {
     "elapsed": 3220,
     "status": "ok",
     "timestamp": 1647423207077,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "ecf83ba2"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator = collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks = [EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb84663",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 563111,
     "status": "error",
     "timestamp": 1647423770176,
     "user": {
      "displayName": "Fiorella Artuso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjI7BMbitgM7kg91MwWnCqwXuMuekaCgxsQRH2b=s64",
      "userId": "16421204404765857445"
     },
     "user_tz": -60
    },
    "id": "11dd32ae",
    "outputId": "545103f0-df4a-4d89-8389-7271c26384dd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8340\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8340' max='8340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8340/8340 27:56, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.048100</td>\n",
       "      <td>4.961988</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.866300</td>\n",
       "      <td>4.735435</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.689700</td>\n",
       "      <td>4.576944</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.534600</td>\n",
       "      <td>4.420834</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.355600</td>\n",
       "      <td>4.185200</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.075000</td>\n",
       "      <td>3.843431</td>\n",
       "      <td>0.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.774100</td>\n",
       "      <td>3.580924</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.557800</td>\n",
       "      <td>3.425163</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.398200</td>\n",
       "      <td>3.280021</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.272800</td>\n",
       "      <td>3.185100</td>\n",
       "      <td>0.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.175500</td>\n",
       "      <td>3.119683</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.095600</td>\n",
       "      <td>3.057008</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.025800</td>\n",
       "      <td>3.000659</td>\n",
       "      <td>0.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.967000</td>\n",
       "      <td>2.964026</td>\n",
       "      <td>0.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.919000</td>\n",
       "      <td>2.921210</td>\n",
       "      <td>0.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.876700</td>\n",
       "      <td>2.888405</td>\n",
       "      <td>0.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.842200</td>\n",
       "      <td>2.865798</td>\n",
       "      <td>0.281800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.815500</td>\n",
       "      <td>2.851800</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.795300</td>\n",
       "      <td>2.839641</td>\n",
       "      <td>0.284800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.783000</td>\n",
       "      <td>2.836536</td>\n",
       "      <td>0.285800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-417\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-417/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-417/pytorch_model.bin\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-834\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-834/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-834/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-417] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1251\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1251/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1251/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-834] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1668\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1668/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1668/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1251] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2085\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2085/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2085/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-1668] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2502\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2502/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2502/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2085] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2919\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2919/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2919/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2502] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3336\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3336/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3336/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-2919] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3753\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3753/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3753/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3336] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4170\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4170/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4170/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-3753] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4587\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4587/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4587/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4170] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5004\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5004/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5004/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-4587] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5421\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5421/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5421/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5004] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5838\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5838/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5838/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5421] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6255\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6255/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6255/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-5838] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6672\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6672/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6672/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6255] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7089\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7089/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7089/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-6672] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7506\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7506/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7506/pytorch_model.bin\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7923\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7923/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7923/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7089] due to args.save_total_limit\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7506] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 96\n",
      "Saving model checkpoint to /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340\n",
      "Configuration saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340/config.json\n",
      "Model weights saved in /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340/pytorch_model.bin\n",
      "Deleting older checkpoint [/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-7923] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340 (score: 0.2858).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8340, training_loss=3.5433929516543015, metrics={'train_runtime': 1687.0445, 'train_samples_per_second': 474.202, 'train_steps_per_second': 4.944, 'total_flos': 7.97681615033618e+16, 'train_loss': 3.5433929516543015, 'epoch': 20.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a363a",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffd5beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37bed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../\"\n",
    "test_path = os.path.join(base_path, \"dataset\", \"finetuning_dataset\", \"strand_execution\", \"test_concrete_execution.csv\")\n",
    "tokenizer_path = os.path.join(base_path, \"tokenizer\")\n",
    "\n",
    "model_name = \"BinBert_strand_execution/checkpoint-12500\"\n",
    "model_path = os.path.join(base_path, \"models\", \"finetuned_models\", \"strand_execution\", model_name)\n",
    "res_filename = os.path.join(base_path, \"results\", \"strand_execution\", model_name.replace(os.sep,\"_\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da287ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binbert_model(best_checkpoint):\n",
    "\n",
    "    print(\"Loading Model ->\", best_checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(best_checkpoint, output_hidden_states=True)\n",
    "\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a99c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2812eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5812bed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /home/jovyan/work/olivetree/final_for_paper/tokenizer/added_tokens.json. We won't load it.\n",
      "loading file /home/jovyan/work/olivetree/final_for_paper/tokenizer/vocab.txt\n",
      "loading file /home/jovyan/work/olivetree/final_for_paper/tokenizer/tokenizer.json\n",
      "loading file None\n",
      "loading file /home/jovyan/work/olivetree/final_for_paper/tokenizer/special_tokens_map.json\n",
      "loading file /home/jovyan/work/olivetree/final_for_paper/tokenizer/tokenizer_config.json\n",
      "loading configuration file /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\",\n",
      "    \"156\": \"LABEL_156\",\n",
      "    \"157\": \"LABEL_157\",\n",
      "    \"158\": \"LABEL_158\",\n",
      "    \"159\": \"LABEL_159\",\n",
      "    \"160\": \"LABEL_160\",\n",
      "    \"161\": \"LABEL_161\",\n",
      "    \"162\": \"LABEL_162\",\n",
      "    \"163\": \"LABEL_163\",\n",
      "    \"164\": \"LABEL_164\",\n",
      "    \"165\": \"LABEL_165\",\n",
      "    \"166\": \"LABEL_166\",\n",
      "    \"167\": \"LABEL_167\",\n",
      "    \"168\": \"LABEL_168\",\n",
      "    \"169\": \"LABEL_169\",\n",
      "    \"170\": \"LABEL_170\",\n",
      "    \"171\": \"LABEL_171\",\n",
      "    \"172\": \"LABEL_172\",\n",
      "    \"173\": \"LABEL_173\",\n",
      "    \"174\": \"LABEL_174\",\n",
      "    \"175\": \"LABEL_175\",\n",
      "    \"176\": \"LABEL_176\",\n",
      "    \"177\": \"LABEL_177\",\n",
      "    \"178\": \"LABEL_178\",\n",
      "    \"179\": \"LABEL_179\",\n",
      "    \"180\": \"LABEL_180\",\n",
      "    \"181\": \"LABEL_181\",\n",
      "    \"182\": \"LABEL_182\",\n",
      "    \"183\": \"LABEL_183\",\n",
      "    \"184\": \"LABEL_184\",\n",
      "    \"185\": \"LABEL_185\",\n",
      "    \"186\": \"LABEL_186\",\n",
      "    \"187\": \"LABEL_187\",\n",
      "    \"188\": \"LABEL_188\",\n",
      "    \"189\": \"LABEL_189\",\n",
      "    \"190\": \"LABEL_190\",\n",
      "    \"191\": \"LABEL_191\",\n",
      "    \"192\": \"LABEL_192\",\n",
      "    \"193\": \"LABEL_193\",\n",
      "    \"194\": \"LABEL_194\",\n",
      "    \"195\": \"LABEL_195\",\n",
      "    \"196\": \"LABEL_196\",\n",
      "    \"197\": \"LABEL_197\",\n",
      "    \"198\": \"LABEL_198\",\n",
      "    \"199\": \"LABEL_199\",\n",
      "    \"200\": \"LABEL_200\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_156\": 156,\n",
      "    \"LABEL_157\": 157,\n",
      "    \"LABEL_158\": 158,\n",
      "    \"LABEL_159\": 159,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_160\": 160,\n",
      "    \"LABEL_161\": 161,\n",
      "    \"LABEL_162\": 162,\n",
      "    \"LABEL_163\": 163,\n",
      "    \"LABEL_164\": 164,\n",
      "    \"LABEL_165\": 165,\n",
      "    \"LABEL_166\": 166,\n",
      "    \"LABEL_167\": 167,\n",
      "    \"LABEL_168\": 168,\n",
      "    \"LABEL_169\": 169,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_170\": 170,\n",
      "    \"LABEL_171\": 171,\n",
      "    \"LABEL_172\": 172,\n",
      "    \"LABEL_173\": 173,\n",
      "    \"LABEL_174\": 174,\n",
      "    \"LABEL_175\": 175,\n",
      "    \"LABEL_176\": 176,\n",
      "    \"LABEL_177\": 177,\n",
      "    \"LABEL_178\": 178,\n",
      "    \"LABEL_179\": 179,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_180\": 180,\n",
      "    \"LABEL_181\": 181,\n",
      "    \"LABEL_182\": 182,\n",
      "    \"LABEL_183\": 183,\n",
      "    \"LABEL_184\": 184,\n",
      "    \"LABEL_185\": 185,\n",
      "    \"LABEL_186\": 186,\n",
      "    \"LABEL_187\": 187,\n",
      "    \"LABEL_188\": 188,\n",
      "    \"LABEL_189\": 189,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_190\": 190,\n",
      "    \"LABEL_191\": 191,\n",
      "    \"LABEL_192\": 192,\n",
      "    \"LABEL_193\": 193,\n",
      "    \"LABEL_194\": 194,\n",
      "    \"LABEL_195\": 195,\n",
      "    \"LABEL_196\": 196,\n",
      "    \"LABEL_197\": 197,\n",
      "    \"LABEL_198\": 198,\n",
      "    \"LABEL_199\": 199,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_200\": 200,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 7814\n",
      "}\n",
      "\n",
      "loading weights file /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model -> /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/jovyan/work/olivetree/final_for_paper/tests/strand_execution/finetuned_models_new/olivetree/normal_strandexec_from_scratch/checkpoint-8340.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "model = load_binbert_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f729c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 8250.29it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = StrandExecDataset(test_path, tokenizer)\n",
    "collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "523451b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_test_accuracy(test_data_loader):\n",
    "    logits = []\n",
    "    references = []\n",
    "    for batch in tqdm(test_data_loader):\n",
    "        for elem in batch:\n",
    "            batch[elem] = batch[elem].to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            pred = model(**batch).logits.cpu().detach().numpy()\n",
    "        ref = batch[\"labels\"].cpu().detach().numpy()\n",
    "        logits.append(pred)\n",
    "        references.append(ref)\n",
    "\n",
    "    predictions = np.concatenate(logits)\n",
    "    references = np.concatenate(references)\n",
    "    accuracy = compute_metrics((predictions, references))[\"accuracy\"]\n",
    "    return accuracy, predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "318ae877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_conf_matrix(predictions, references, res_filename, labels):\n",
    "\n",
    "    cm = confusion_matrix(references, np.argmax(predictions, axis=-1))\n",
    "    \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax, cmap=\"Blues\")\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(f'Confusion Matrix with Accuracy {round(accuracy,4)}')\n",
    "    ax.xaxis.set_xticks(labels)\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_yticks(labels)\n",
    "    res_filename =  f\"{res_filename}_acc_{round(accuracy,4)}.png\"\n",
    "    print(res_filename)\n",
    "    plt.savefig(res_filename, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ce229b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a70a8310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 16.09it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy, predictions, references = compute_test_accuracy(test_data_loader)\n",
    "# compute_test_conf_matrix(predictions, references, res_filename, sorted(list(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2959a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_filename =  f\"{res_filename}_acc_{round(accuracy,4)}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fb00121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82        46\n",
      "           1       0.86      0.90      0.88        71\n",
      "           2       0.40      0.55      0.46        53\n",
      "           3       0.31      0.26      0.28        43\n",
      "           4       0.28      0.49      0.36        47\n",
      "           5       0.32      0.52      0.40        61\n",
      "           6       0.52      0.44      0.47        32\n",
      "           7       0.44      0.35      0.39        49\n",
      "           8       0.21      0.43      0.29        46\n",
      "           9       0.43      0.24      0.31        42\n",
      "          10       0.14      0.18      0.16        44\n",
      "          11       0.37      0.27      0.31        52\n",
      "          12       0.13      0.13      0.13        38\n",
      "          13       0.40      0.33      0.36        49\n",
      "          14       0.29      0.43      0.34        42\n",
      "          15       0.35      0.29      0.32        45\n",
      "          16       0.32      0.51      0.39        55\n",
      "          17       0.53      0.36      0.43        45\n",
      "          18       0.43      0.31      0.36        39\n",
      "          19       0.24      0.19      0.22        36\n",
      "          20       0.39      0.41      0.40        37\n",
      "          21       0.31      0.21      0.25        38\n",
      "          22       0.12      0.19      0.15        36\n",
      "          23       0.29      0.27      0.28        45\n",
      "          24       0.53      0.35      0.42        49\n",
      "          25       0.31      0.20      0.24        41\n",
      "          26       0.48      0.39      0.43        51\n",
      "          27       0.33      0.36      0.34        39\n",
      "          28       0.34      0.40      0.37        45\n",
      "          29       0.33      0.31      0.32        49\n",
      "          30       0.34      0.27      0.30        44\n",
      "          31       0.16      0.28      0.21        32\n",
      "          32       0.22      0.23      0.22        44\n",
      "          33       0.34      0.30      0.32        50\n",
      "          34       0.35      0.23      0.27        40\n",
      "          35       0.39      0.49      0.43        39\n",
      "          36       0.50      0.38      0.43        45\n",
      "          37       0.36      0.45      0.40        33\n",
      "          38       0.28      0.33      0.30        40\n",
      "          39       0.42      0.27      0.33        37\n",
      "          40       0.27      0.35      0.30        34\n",
      "          41       0.39      0.28      0.32        40\n",
      "          42       0.45      0.31      0.37        42\n",
      "          43       0.31      0.21      0.25        43\n",
      "          44       0.25      0.28      0.27        47\n",
      "          45       0.27      0.33      0.30        39\n",
      "          46       0.35      0.33      0.34        40\n",
      "          47       0.22      0.31      0.26        36\n",
      "          48       0.21      0.28      0.24        43\n",
      "          49       0.34      0.43      0.38        42\n",
      "          50       0.30      0.15      0.20        41\n",
      "          51       0.30      0.26      0.28        35\n",
      "          52       0.19      0.16      0.18        31\n",
      "          53       0.33      0.29      0.31        28\n",
      "          54       0.21      0.23      0.22        26\n",
      "          55       0.26      0.38      0.31        40\n",
      "          56       0.34      0.37      0.35        38\n",
      "          57       0.33      0.27      0.30        44\n",
      "          58       0.21      0.17      0.19        40\n",
      "          59       0.35      0.29      0.32        41\n",
      "          60       0.38      0.31      0.34        35\n",
      "          61       0.41      0.33      0.36        40\n",
      "          62       0.25      0.29      0.27        45\n",
      "          63       0.25      0.36      0.29        53\n",
      "          64       0.33      0.31      0.32        55\n",
      "          65       0.36      0.24      0.29        37\n",
      "          66       0.21      0.19      0.20        26\n",
      "          67       0.32      0.18      0.23        45\n",
      "          68       0.51      0.54      0.52        41\n",
      "          69       0.15      0.20      0.17        25\n",
      "          70       0.44      0.43      0.44        37\n",
      "          71       0.28      0.30      0.29        44\n",
      "          72       0.28      0.32      0.30        34\n",
      "          73       0.37      0.24      0.29        42\n",
      "          74       0.49      0.43      0.46        51\n",
      "          75       0.28      0.34      0.31        41\n",
      "          76       0.29      0.34      0.31        32\n",
      "          77       0.42      0.37      0.39        41\n",
      "          78       0.35      0.16      0.22        43\n",
      "          79       0.25      0.31      0.28        32\n",
      "          80       0.36      0.35      0.35        40\n",
      "          81       0.29      0.21      0.24        34\n",
      "          82       0.22      0.28      0.25        46\n",
      "          83       0.31      0.24      0.27        46\n",
      "          84       0.41      0.26      0.32        47\n",
      "          85       0.39      0.30      0.34        47\n",
      "          86       0.22      0.27      0.24        51\n",
      "          87       0.37      0.29      0.33        45\n",
      "          88       0.39      0.25      0.31        44\n",
      "          89       0.28      0.35      0.31        34\n",
      "          90       0.33      0.46      0.39        35\n",
      "          91       0.41      0.40      0.41        35\n",
      "          92       0.41      0.33      0.36        40\n",
      "          93       0.30      0.15      0.20        41\n",
      "          94       0.29      0.59      0.39        39\n",
      "          95       0.23      0.45      0.30        65\n",
      "          96       0.29      0.33      0.31        36\n",
      "          97       0.20      0.20      0.20        30\n",
      "          98       0.27      0.30      0.29        33\n",
      "          99       0.23      0.23      0.23        40\n",
      "         100       0.25      0.47      0.33        32\n",
      "         101       0.21      0.62      0.31        21\n",
      "         102       0.00      0.00      0.00        22\n",
      "         103       0.00      0.00      0.00        25\n",
      "         104       0.00      0.00      0.00        17\n",
      "         105       0.00      0.00      0.00        15\n",
      "         106       0.20      0.05      0.07        22\n",
      "         107       0.07      0.13      0.09        15\n",
      "         108       0.00      0.00      0.00        15\n",
      "         109       0.00      0.00      0.00        19\n",
      "         110       0.00      0.00      0.00        10\n",
      "         111       0.04      0.10      0.06        21\n",
      "         112       0.33      0.06      0.10        17\n",
      "         113       0.00      0.00      0.00        14\n",
      "         114       0.00      0.00      0.00        29\n",
      "         115       0.00      0.00      0.00        12\n",
      "         116       0.00      0.00      0.00         9\n",
      "         117       0.00      0.00      0.00        12\n",
      "         118       0.00      0.00      0.00        12\n",
      "         119       0.00      0.00      0.00        16\n",
      "         120       0.07      0.07      0.07        14\n",
      "         121       0.00      0.00      0.00        11\n",
      "         122       0.00      0.00      0.00        12\n",
      "         123       0.06      0.03      0.04        31\n",
      "         124       0.00      0.00      0.00        20\n",
      "         125       0.00      0.00      0.00        14\n",
      "         126       0.00      0.00      0.00        27\n",
      "         127       0.14      0.63      0.23        35\n",
      "         128       0.00      0.00      0.00        10\n",
      "         129       0.00      0.00      0.00         2\n",
      "         130       0.00      0.00      0.00         4\n",
      "         131       0.00      0.00      0.00         6\n",
      "         132       0.04      0.33      0.08         6\n",
      "         133       0.00      0.00      0.00         5\n",
      "         134       0.00      0.00      0.00         3\n",
      "         135       0.00      0.00      0.00         5\n",
      "         136       0.00      0.00      0.00        10\n",
      "         137       0.00      0.00      0.00         7\n",
      "         138       0.00      0.00      0.00         2\n",
      "         139       0.00      0.00      0.00         8\n",
      "         140       0.00      0.00      0.00         8\n",
      "         141       0.00      0.00      0.00         2\n",
      "         142       0.00      0.00      0.00         5\n",
      "         143       0.00      0.00      0.00         7\n",
      "         144       0.00      0.00      0.00         6\n",
      "         145       0.00      0.00      0.00         3\n",
      "         146       0.00      0.00      0.00         4\n",
      "         147       0.00      0.00      0.00         7\n",
      "         148       0.00      0.00      0.00         6\n",
      "         149       0.00      0.00      0.00         6\n",
      "         150       0.00      0.00      0.00         2\n",
      "         151       0.03      0.60      0.06         5\n",
      "         152       0.00      0.00      0.00         4\n",
      "         153       0.00      0.00      0.00         4\n",
      "         154       0.00      0.00      0.00         7\n",
      "         155       0.00      0.00      0.00         5\n",
      "         156       0.00      0.00      0.00         6\n",
      "         157       0.00      0.00      0.00         1\n",
      "         158       0.00      0.00      0.00         6\n",
      "         159       0.00      0.00      0.00         4\n",
      "         160       0.00      0.00      0.00         7\n",
      "         161       0.00      0.00      0.00         5\n",
      "         162       0.00      0.00      0.00         3\n",
      "         163       0.00      0.00      0.00         5\n",
      "         164       0.00      0.00      0.00         6\n",
      "         165       0.00      0.00      0.00         2\n",
      "         166       0.00      0.00      0.00         3\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         5\n",
      "         169       0.00      0.00      0.00         4\n",
      "         170       0.00      0.00      0.00         4\n",
      "         171       0.00      0.00      0.00         1\n",
      "         172       0.00      0.00      0.00         4\n",
      "         173       0.00      0.00      0.00         6\n",
      "         174       0.00      0.00      0.00         3\n",
      "         175       0.00      0.00      0.00         3\n",
      "         176       0.00      0.00      0.00         4\n",
      "         177       0.00      0.00      0.00         4\n",
      "         178       0.00      0.00      0.00         6\n",
      "         179       0.00      0.00      0.00         1\n",
      "         180       0.00      0.00      0.00         3\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.00      0.00      0.00         1\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       0.00      0.00      0.00         5\n",
      "         185       0.00      0.00      0.00         4\n",
      "         186       0.00      0.00      0.00         5\n",
      "         187       0.00      0.00      0.00         6\n",
      "         188       0.00      0.00      0.00         2\n",
      "         189       0.00      0.00      0.00         2\n",
      "         190       0.00      0.00      0.00         1\n",
      "         191       0.00      0.00      0.00         3\n",
      "         192       0.00      0.00      0.00         6\n",
      "         193       0.00      0.00      0.00         3\n",
      "         194       0.00      0.00      0.00         6\n",
      "         195       0.00      0.00      0.00         4\n",
      "         196       0.00      0.00      0.00         5\n",
      "         197       0.00      0.00      0.00         1\n",
      "         198       0.00      0.00      0.00         1\n",
      "         199       0.00      0.00      0.00         2\n",
      "         200       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.29      5000\n",
      "   macro avg       0.17      0.18      0.17      5000\n",
      "weighted avg       0.29      0.29      0.28      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "a = classification_report(references, np.argmax(predictions,axis=1))\n",
    "with open(res_filename, \"w\") as f:\n",
    "    f.write(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe351da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "compiler_provenance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}