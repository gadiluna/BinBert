{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52270e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`fused_weight_gradient_mlp_cuda` module not found. gradient accumulation fusion with weight gradient computation disabled.\n",
      "2022-06-09 09:13:39.860317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformer tokenizer imports\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Transformers Bert model\n",
    "from transformers import BertModel, BertForPreTraining, Trainer, TrainingArguments, EarlyStoppingCallback, BertConfig\n",
    "\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1986ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU settings\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9063720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "    # load tokenizer from dict\n",
    "    tokenizer =  BertTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b75199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, tokenizer):\n",
    "        \n",
    "        self.data_store = []\n",
    "        df = pd.read_csv(dataset_path, sep=\"\\t\").fillna('')\n",
    "        #self.samples = df.iloc[:, [2, 5, 6]]\n",
    "        self.samples = df[[\"ot_func1\", \"ot_func2\", \"label\"]]\n",
    "        self.tokenizer = tokenizer\n",
    "            \n",
    "        self.__init_structures()\n",
    "         \n",
    "    def __init_structures(self):\n",
    "        \n",
    "        for first_asm, second_asm, label in tqdm(self.samples.values):\n",
    "            first_asm_example = self.tokenizer(text=\" \".join(first_asm.split(\" NEXT_I \")), truncation=True, max_length=MAX_SEQ_LEN)\n",
    "            second_asm_example = self.tokenizer(text=\" \".join(second_asm.split(\" NEXT_I \")), truncation=True, max_length=MAX_SEQ_LEN)\n",
    "            \n",
    "            example = {\"first\": first_asm_example,\n",
    "                      \"second\": second_asm_example,\n",
    "                      \"label\": label}\n",
    "            \n",
    "            self.data_store.append(example)\n",
    "            \n",
    "        random.shuffle(self.data_store)\n",
    "                \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_store)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.data_store[idx]\n",
    "    \n",
    "    def save_to_file(self,save_file):\n",
    "        torch.save(self.data_store, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bf289f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AsmDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_path, val_path, test_path, batch_size, tokenizer):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        if stage == 'fit':\n",
    "            self.train_dataset = SimDataset(self.train_path, self.tokenizer,)\n",
    "            self.val_dataset   = SimDataset(self.val_path, self.tokenizer)\n",
    "            \n",
    "        elif stage == 'test':\n",
    "            self.test_dataset = SimDataset(self.test_path, self.tokenizer)\n",
    "            \n",
    "    def __pad(self, samples):\n",
    "        \n",
    "        all_tokens_ids = []\n",
    "        all_masks = []\n",
    "        \n",
    "        max_tok_batch = max([len(block[\"input_ids\"]) for block in samples])\n",
    "        \n",
    "        for block in samples:\n",
    "             \n",
    "            num_pad_inst = max_tok_batch - len(block[\"input_ids\"])\n",
    "            all_tokens_ids.append(block[\"input_ids\"] + [self.tokenizer.pad_token_id] * num_pad_inst)\n",
    "            all_masks.append(block[\"attention_mask\"] + [0] * num_pad_inst)\n",
    "            \n",
    "        return {\"input_ids\": torch.tensor(all_tokens_ids, device=\"cuda\"),\n",
    "               \"attention_mask\": torch.tensor(all_masks, device=\"cuda\")}\n",
    "        \n",
    "\n",
    "    def collate_with_padding(self, batch):\n",
    "        \n",
    "        first_p  = [elem[\"first\"] for elem in batch]\n",
    "        second_p = [elem[\"second\"] for elem in batch]\n",
    "        batch_labels = [elem[\"label\"] if elem[\"label\"] else -1 for elem in batch]\n",
    "        \n",
    "        first_p = self.__pad(first_p)\n",
    "        second_p = self.__pad(second_p)\n",
    "        \n",
    "        batch_result = {\"first\": first_p,\n",
    "                        \"second\": second_p,\n",
    "                        \"label\": torch.tensor(batch_labels, device=\"cuda\")}\n",
    "        \n",
    "        return batch_result\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs):    \n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          collate_fn=self.collate_with_padding)\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          collate_fn=self.collate_with_padding)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          collate_fn=self.collate_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82e3b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = \"../../\"\n",
    "prt_model = os.path.join(base_path, \"models\", \"pretraining_model\", \"checkpoint-67246\")\n",
    "train_path = os.path.join(base_path, \"dataset\", \"finetuning_dataset\", \"similarity\", \"functions\", \"train_functions_similarity_triplets.csv\")\n",
    "val_path = os.path.join(base_path, \"dataset\", \"finetuning_dataset\", \"similarity\", \"functions\", \"val_functions_similarity_triplets.csv\")\n",
    "tokenizer_path = os.path.join(base_path, \"tokenizer\")\n",
    "\n",
    "model_name =  f\"BinBert_function_similarity\"\n",
    "output_model_path = os.path.join(base_path, \"models\", \"finetuned_models\", \"similarity\", \"functions\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ec1ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from_scratch = False\n",
    "\n",
    "LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 32\n",
    "DATA_LOADER_NUM_WORKERS = 4\n",
    "PATIENCE = 3\n",
    "\n",
    "#models\n",
    "BXSMAL=\"bert_xsmall\"\n",
    "BSMAL=\"bert_small\"\n",
    "BNORM=\"bert_normal\"\n",
    "BLARG=\"bert_larg\"\n",
    "\n",
    "MODEL=BNORM\n",
    "\n",
    "if MODEL == BXSMAL:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 128\n",
    "    INTERMEDIATE_SIZE = 1024\n",
    "    NUM_ATTENTION_HEADS = 8\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "\n",
    "if MODEL == BSMAL:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 512\n",
    "    INTERMEDIATE_SIZE = 2048\n",
    "    NUM_ATTENTION_HEADS = 8\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "    \n",
    "if MODEL == BNORM:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 768\n",
    "    INTERMEDIATE_SIZE = 3072\n",
    "    NUM_ATTENTION_HEADS = 12\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2    \n",
    "    \n",
    "if MODEL == BLARG:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 1024\n",
    "    INTERMEDIATE_SIZE = 4096\n",
    "    NUM_ATTENTION_HEADS = 16\n",
    "    NUM_HIDDEN_LAYERS = 24\n",
    "    TYPE_VOCAB_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d57da4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SiameseFinenuting(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_path, batch_size, vocab=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Model\n",
    "        if from_scratch:\n",
    "            print(\"From scratch\")\n",
    "            config = BertConfig(\n",
    "                vocab_size = len(vocab),\n",
    "                max_position_embeddings = MAX_POSITION_EMBEDDINGS,\n",
    "                hidden_size = HIDDEN_SIZE,\n",
    "                intermediate_size = INTERMEDIATE_SIZE,\n",
    "                num_attention_heads = NUM_ATTENTION_HEADS,\n",
    "                num_hidden_layers = NUM_HIDDEN_LAYERS,\n",
    "                type_vocab_size = TYPE_VOCAB_SIZE,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            self.model = BertModel(config=config)\n",
    "        else:\n",
    "            self.model = BertModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "        \n",
    "        self.cosine = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "        # Criterion\n",
    "        # self.loss = torch.nn.MSELoss()\n",
    "        self.loss = torch.nn.CosineEmbeddingLoss()\n",
    "        \n",
    "        # metrics\n",
    "        self.train_auc = torchmetrics.AUROC()\n",
    "        self.val_auc = torchmetrics.AUROC()\n",
    "        # metrics = MetricCollection([Accuracy(), Precision(), Recall()])\n",
    "        # self.train_metrics = metrics.clone(prefix='train_')\n",
    "        # self.valid_metrics = metrics.clone(prefix='val_')\n",
    "        \n",
    "    def forward(self, pairs_asm_input):\n",
    "        \n",
    "        first = pairs_asm_input[\"first\"]\n",
    "        second = pairs_asm_input[\"second\"]\n",
    "        labels = pairs_asm_input[\"label\"]\n",
    "        \n",
    "        # take hidden state of the CLS token in the last layer as embedding\n",
    "\n",
    "        #first_embeddings = self.model(**first).hidden_states[-1][:,0]\n",
    "        #second_embeddings = self.model(**second).hidden_states[-1][:,0]\n",
    "        \n",
    "        first_output = self.model(**first)\n",
    "        second_output = self.model(**second)\n",
    "        \n",
    "        first_hidden_states = first_output.hidden_states[-1]\n",
    "        second_hidden_states = second_output.hidden_states[-1]\n",
    "\n",
    "        first_masks = first['attention_mask']\n",
    "        second_masks = second['attention_mask']\n",
    "\n",
    "        first_partial_mul = first_hidden_states * first_masks.unsqueeze(-1)\n",
    "        second_partial_mul = second_hidden_states * second_masks.unsqueeze(-1)\n",
    "        \n",
    "        first_partial_sum = torch.sum(first_partial_mul, dim=1)\n",
    "        second_partial_sum = torch.sum(second_partial_mul, dim=1)\n",
    "        \n",
    "        first_n = torch.sum(first_masks, dim=1)\n",
    "        second_n = torch.sum(second_masks, dim=1)\n",
    "        \n",
    "        first_embeddings = first_partial_sum / first_n.unsqueeze(-1)\n",
    "        second_embeddings = second_partial_sum / second_n.unsqueeze(-1)\n",
    "\n",
    "        cosines = self.cosine(first_embeddings, second_embeddings)\n",
    "        \n",
    "        result = dict()\n",
    "        result['prediction'] = cosines\n",
    "        result['labels'] = labels\n",
    "        \n",
    "        result['first_embedding'] = first_embeddings\n",
    "        result['second_embedding'] = second_embeddings\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        forward_output = self.forward(batch)\n",
    "        \n",
    "        prediction = forward_output[\"prediction\"]\n",
    "        labels = forward_output[\"labels\"]\n",
    "        \n",
    "        # loss = self.loss(prediction, labels.float())\n",
    "        loss = self.loss(forward_output['first_embedding'], forward_output['second_embedding'], labels)\n",
    "        \n",
    "        l2 = labels.clone()\n",
    "        l2[l2==-1]=0\n",
    "        # self.train_auc.update(prediction, l2)\n",
    "        m = self.train_auc(prediction, l2)\n",
    "         \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        self.log('train_auc', m, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return {\"loss\":loss,\n",
    "                \"train_auc\":m}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        forward_output = self.forward(batch)\n",
    "        \n",
    "        prediction = forward_output[\"prediction\"]\n",
    "        labels = forward_output[\"labels\"]\n",
    "        \n",
    "        # loss = self.loss(prediction, labels.float())\n",
    "        loss = self.loss(forward_output['first_embedding'], forward_output['second_embedding'], labels)\n",
    "        \n",
    "        l2 = labels.clone()\n",
    "        l2[l2==-1]=0\n",
    "        # self.val_auc.update(prediction, l2)\n",
    "        m = self.val_auc(prediction, l2)\n",
    "    \n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        self.log('val_auc', m, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return {\"loss\":loss,\n",
    "                \"val_auc\":m}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        forward_output = self.forward(batch)\n",
    "        \n",
    "        prediction = forward_output[\"prediction\"]\n",
    "        labels = forward_output[\"labels\"]\n",
    "        \n",
    "        # loss = self.loss(prediction, labels)\n",
    "        \n",
    "        loss = self.loss(forward_output['first_embedding'], forward_output['second_embedding'], labels)\n",
    "        # metrics = self.train_metrics(prediction, labels)\n",
    "        metrics = dict()\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=self. batch_size)\n",
    "        # self.log(\"train_Accuracy\", metrics[\"train_Accuracy\"], on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        metrics[\"loss\"] = loss\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066a467d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trainer(ckpt_dir):\n",
    "\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\n",
    "        # nitor='val_loss',\n",
    "        monitor='val_auc',\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        mode='max', # wheter we want to maximize (max) or minimize the \"monitor\" value.\n",
    "    )\n",
    "\n",
    "    check_point_callback = pl.callbacks.ModelCheckpoint(\n",
    "        # nitor='val_loss',\n",
    "        monitor='val_auc',\n",
    "        verbose=True,\n",
    "        save_top_k=1,\n",
    "        mode='max', # wheter we want to maximize (max) or minimize the \"monitor\" value.\n",
    "        dirpath=ckpt_dir,\n",
    "        filename='{epoch}-{val_auc:.4f}',\n",
    "        # save_weights_only = True\n",
    "    )\n",
    "\n",
    "\n",
    "    # the PyTorch Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=NUM_TRAIN_EPOCHS,\n",
    "        gpus=1,\n",
    "        progress_bar_refresh_rate=5,\n",
    "        # callbacks=[early_stopping, check_point_callback]\n",
    "        callbacks=[check_point_callback]\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4954a9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "data_module = AsmDataModule(train_path, val_path, None, BATCH_SIZE, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9418073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/olivetree/final_for_paper/models/next_sentence_prediction_bert_normal_mask30/checkpoint-67246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/work/olivetree/final_for_paper/models/next_sentence_prediction_bert_normal_mask30/checkpoint-67246 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(prt_model)\n",
    "simaese_model = SiameseFinenuting(prt_model, BATCH_SIZE, vocab=tokenizer.vocab if from_scratch else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b25bc66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=5)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = get_trainer(ckpt_dir=output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "616ef7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18804/18804 [00:27<00:00, 674.99it/s]\n",
      "100%|██████████| 1674/1674 [00:02<00:00, 709.14it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | model     | BertModel           | 92.0 M\n",
      "1 | cosine    | CosineSimilarity    | 0     \n",
      "2 | loss      | CosineEmbeddingLoss | 0     \n",
      "3 | train_auc | AUROC               | 0     \n",
      "4 | val_auc   | AUROC               | 0     \n",
      "--------------------------------------------------\n",
      "92.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "92.0 M    Total params\n",
      "368.176   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971a2491aff74cb2b280cb1764cc1992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1175: val_auc reached 0.97734 (best 0.97734), saving model to \"/home/jovyan/work/olivetree/final_for_paper/tests/similarity/functions/fine_tuned_models/olivetree/nsp_normal_mask30_avg_cos_emb_loss/epoch=0-val_auc=0.9773.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2351: val_auc reached 0.98136 (best 0.98136), saving model to \"/home/jovyan/work/olivetree/final_for_paper/tests/similarity/functions/fine_tuned_models/olivetree/nsp_normal_mask30_avg_cos_emb_loss/epoch=1-val_auc=0.9814.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3527: val_auc reached 0.98239 (best 0.98239), saving model to \"/home/jovyan/work/olivetree/final_for_paper/tests/similarity/functions/fine_tuned_models/olivetree/nsp_normal_mask30_avg_cos_emb_loss/epoch=2-val_auc=0.9824.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 4703: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 5879: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 7055: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 8231: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 9407: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 10583: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 11759: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 12935: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 14111: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 15287: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 16463: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 17639: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 18815: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 19991: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 21167: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 22343: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 23519: val_auc was not in top 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=simaese_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3575baaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##To convert Model Output\n",
    "config = BertConfig(\n",
    "            vocab_size = len(tokenizer.vocab),\n",
    "            max_position_embeddings = MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_size = HIDDEN_SIZE,\n",
    "            intermediate_size = INTERMEDIATE_SIZE,\n",
    "            num_attention_heads = NUM_ATTENTION_HEADS,\n",
    "            num_hidden_layers = NUM_HIDDEN_LAYERS,\n",
    "            type_vocab_size = TYPE_VOCAB_SIZE)\n",
    "model = BertModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef4a585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_torch = torch.load(os.path.join(output_model_path, \"epoch=2-val_auc=0.9824.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52546dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dict = dict()\n",
    "for k in model_torch[\"state_dict\"]:\n",
    "    new_dict[k.replace(\"model.\",\"\")] = model_torch[\"state_dict\"][k]\n",
    "del model_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa79f431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "917ffc49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(os.path.join(output_model_path, \"epoch-2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac2ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}