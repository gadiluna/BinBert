{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b682dd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformer tokenizer imports\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Transformers Bert model\n",
    "from transformers import BertModel, BertForPreTraining, Trainer, TrainingArguments, EarlyStoppingCallback, BertConfig\n",
    "\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d0ba9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4076/258768398.py:9: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(datetime.now())\n"
     ]
    }
   ],
   "source": [
    "# GPU settings\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202e49fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "    # load tokenizer from dict\n",
    "    tokenizer =  BertTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2d5db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, tokenizer):\n",
    "        \n",
    "        self.data_store = []\n",
    "        df = pd.read_csv(dataset_path, sep=\"\\t\").fillna('')\n",
    "        #self.samples = df.iloc[:, [2, 5, 6]]\n",
    "        self.samples = df[[\"ot_strand_anchor\", \"ot_strand_pos\", \"ot_strand_neg\"]]\n",
    "        self.tokenizer = tokenizer\n",
    "            \n",
    "        self.__init_structures()\n",
    "         \n",
    "    def __init_structures(self):\n",
    "        \n",
    "        for first_asm, second_asm, third_asm in tqdm(self.samples.values):\n",
    "            first_asm_example = self.tokenizer(text=\" \".join(first_asm.split(\" NEXT_I \")), truncation=True, max_length=MAX_SEQ_LEN)\n",
    "            second_asm_example = self.tokenizer(text=\" \".join(second_asm.split(\" NEXT_I \")), truncation=True, max_length=MAX_SEQ_LEN)\n",
    "            third_asm_example = self.tokenizer(text=\" \".join(third_asm.split(\" NEXT_I \")), truncation=True, max_length=MAX_SEQ_LEN)\n",
    "            \n",
    "            example = {\"anchor\": first_asm_example,\n",
    "                      \"pos\": second_asm_example,\n",
    "                      \"neg\": third_asm_example}\n",
    "            \n",
    "            self.data_store.append(example)\n",
    "            \n",
    "        random.shuffle(self.data_store)\n",
    "                \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_store)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.data_store[idx]\n",
    "    \n",
    "    def save_to_file(self,save_file):\n",
    "        torch.save(self.data_store, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4079d2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AsmDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_path, val_path, test_path, batch_size, tokenizer):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        if stage == 'fit':\n",
    "            self.train_dataset = SimDataset(self.train_path, self.tokenizer,)\n",
    "            self.val_dataset   = SimDataset(self.val_path, self.tokenizer)\n",
    "            \n",
    "        elif stage == 'test':\n",
    "            self.test_dataset = SimDataset(self.test_path, self.tokenizer)\n",
    "            \n",
    "    def __pad(self, samples):\n",
    "        \n",
    "        all_tokens_ids = []\n",
    "        all_masks = []\n",
    "        \n",
    "        max_tok_batch = max([len(block[\"input_ids\"]) for block in samples])\n",
    "        \n",
    "        for block in samples:\n",
    "             \n",
    "            num_pad_inst = max_tok_batch - len(block[\"input_ids\"])\n",
    "            all_tokens_ids.append(block[\"input_ids\"] + [self.tokenizer.pad_token_id] * num_pad_inst)\n",
    "            all_masks.append(block[\"attention_mask\"] + [0] * num_pad_inst)\n",
    "            \n",
    "        return {\"input_ids\": torch.tensor(all_tokens_ids, device=\"cuda\"),\n",
    "               \"attention_mask\": torch.tensor(all_masks, device=\"cuda\")}\n",
    "        \n",
    "\n",
    "    def collate_with_padding(self, batch):\n",
    "        \n",
    "        first_p  = [elem[\"anchor\"] for elem in batch]\n",
    "        second_p = [elem[\"pos\"] for elem in batch]\n",
    "        third_p = [elem[\"neg\"] for elem in batch]\n",
    "        \n",
    "        first_p = self.__pad(first_p)\n",
    "        second_p = self.__pad(second_p)\n",
    "        third_p = self.__pad(third_p)\n",
    "        \n",
    "        batch_result = {\"anchor\": first_p,\n",
    "                        \"pos\": second_p,\n",
    "                        \"neg\": third_p}\n",
    "        \n",
    "        return batch_result\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs):    \n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          collate_fn=self.collate_with_padding)\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          collate_fn=self.collate_with_padding)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size, \n",
    "                          collate_fn=self.collate_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d77314c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = \"../../\"\n",
    "prt_model = os.path.join(base_path, \"models\", \"pretraining_model\", \"checkpoint-67246\")\n",
    "train_path = os.path.join(base_path, \"dataset\", \"finetuning_dataset\", \"similarity\", \"strands\", \"train_strands_similarity_triplets.csv\")\n",
    "val_path = os.path.join(base_path, \"dataset\", \"finetuning_dataset\", \"similarity\", \"strands\", \"val_strands_similarity_triplets.csv\")\n",
    "tokenizer_path = os.path.join(base_path, \"tokenizer\")\n",
    "\n",
    "model_name =  f\"BinBert_strand_similarity\"\n",
    "output_model_path = os.path.join(base_path, \"models\", \"finetuned_models\", \"similarity\", \"strands\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff055bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from_scratch = True\n",
    "\n",
    "LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 16\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 16\n",
    "DATA_LOADER_NUM_WORKERS = 4\n",
    "PATIENCE = 3\n",
    "\n",
    "#models\n",
    "BXSMAL=\"bert_xsmall\"\n",
    "BSMAL=\"bert_small\"\n",
    "BNORM=\"bert_normal\"\n",
    "BLARG=\"bert_larg\"\n",
    "\n",
    "MODEL=BNORM\n",
    "\n",
    "if MODEL == BXSMAL:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 128\n",
    "    INTERMEDIATE_SIZE = 1024\n",
    "    NUM_ATTENTION_HEADS = 8\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "\n",
    "if MODEL == BSMAL:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 512\n",
    "    INTERMEDIATE_SIZE = 2048\n",
    "    NUM_ATTENTION_HEADS = 8\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "    \n",
    "if MODEL == BNORM:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 768\n",
    "    INTERMEDIATE_SIZE = 3072\n",
    "    NUM_ATTENTION_HEADS = 12\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2    \n",
    "    \n",
    "if MODEL == BLARG:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 1024\n",
    "    INTERMEDIATE_SIZE = 4096\n",
    "    NUM_ATTENTION_HEADS = 16\n",
    "    NUM_HIDDEN_LAYERS = 24\n",
    "    TYPE_VOCAB_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bae4f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SiameseFinenuting(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_path, batch_size, vocab=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Model\n",
    "        if from_scratch:\n",
    "            print(\"From scratch\")\n",
    "            config = BertConfig(\n",
    "                vocab_size = len(vocab),\n",
    "                max_position_embeddings = MAX_POSITION_EMBEDDINGS,\n",
    "                hidden_size = HIDDEN_SIZE,\n",
    "                intermediate_size = INTERMEDIATE_SIZE,\n",
    "                num_attention_heads = NUM_ATTENTION_HEADS,\n",
    "                num_hidden_layers = NUM_HIDDEN_LAYERS,\n",
    "                type_vocab_size = TYPE_VOCAB_SIZE,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            self.model = BertModel(config=config)\n",
    "        else:\n",
    "            self.model = BertModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "        \n",
    "        # criterion\n",
    "        self.triplet_loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "        \n",
    "        self.cosine = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.train_auc = torchmetrics.AUROC()\n",
    "        self.val_auc = torchmetrics.AUROC()\n",
    "\n",
    "        \n",
    "    def forward(self, pairs_asm_input):\n",
    "        \n",
    "        first = pairs_asm_input[\"anchor\"]\n",
    "        second = pairs_asm_input[\"pos\"]\n",
    "        third = pairs_asm_input[\"neg\"]\n",
    "        \n",
    "        first_output = self.model(**first)\n",
    "        second_output = self.model(**second)\n",
    "        third_output = self.model(**third)\n",
    "        \n",
    "        first_hidden_states = first_output.hidden_states[-1]\n",
    "        second_hidden_states = second_output.hidden_states[-1]\n",
    "        third_hidden_states = third_output.hidden_states[-1]\n",
    "\n",
    "        first_masks = first['attention_mask']\n",
    "        second_masks = second['attention_mask']\n",
    "        third_masks = third['attention_mask']\n",
    "\n",
    "        first_partial_mul = first_hidden_states * first_masks.unsqueeze(-1)\n",
    "        second_partial_mul = second_hidden_states * second_masks.unsqueeze(-1)\n",
    "        third_partial_mul = third_hidden_states * third_masks.unsqueeze(-1)\n",
    "        \n",
    "        first_partial_sum = torch.sum(first_partial_mul, dim=1)\n",
    "        second_partial_sum = torch.sum(second_partial_mul, dim=1)\n",
    "        third_partial_sum = torch.sum(third_partial_mul, dim=1)\n",
    "        \n",
    "        first_n = torch.sum(first_masks, dim=1)\n",
    "        second_n = torch.sum(second_masks, dim=1)\n",
    "        third_n = torch.sum(third_masks, dim=1)\n",
    "        \n",
    "        first_embeddings = first_partial_sum / first_n.unsqueeze(-1)\n",
    "        second_embeddings = second_partial_sum / second_n.unsqueeze(-1)\n",
    "        third_embeddings = third_partial_sum / third_n.unsqueeze(-1)\n",
    "        \n",
    "        cosines_pos = self.cosine(first_embeddings, second_embeddings)\n",
    "        cosines_neg = self.cosine(first_embeddings, third_embeddings)\n",
    "        \n",
    "        result = dict()\n",
    "        result['first_embedding'] = first_embeddings\n",
    "        result['second_embedding'] = second_embeddings\n",
    "        result['third_embedding'] = third_embeddings\n",
    "        result['cos_pos'] = cosines_pos\n",
    "        result['cos_neg'] = cosines_neg\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        forward_output = self.forward(batch)\n",
    "        \n",
    "        loss = self.triplet_loss(forward_output['first_embedding'], \n",
    "                                 forward_output['second_embedding'], \n",
    "                                 forward_output['third_embedding'])\n",
    "         \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        pred_pos = forward_output['cos_pos']\n",
    "        pred_neg = forward_output['cos_neg']\n",
    "        labels_pos = torch.tensor([1]*pred_pos.shape[0]).to(\"cuda\")\n",
    "        labels_neg = torch.tensor([0]*pred_neg.shape[0]).to(\"cuda\")\n",
    "        \n",
    "        m = self.train_auc(torch.cat((pred_pos,pred_neg),0), torch.cat((labels_pos,labels_neg),0))\n",
    "        \n",
    "        self.log('train_auc', m, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return {\"loss\":loss, 'train_auc':m}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        forward_output = self.forward(batch)\n",
    "        \n",
    "        loss = self.triplet_loss(forward_output['first_embedding'], \n",
    "                                 forward_output['second_embedding'], \n",
    "                                 forward_output['third_embedding'])\n",
    "         \n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        pred_pos = forward_output['cos_pos']\n",
    "        pred_neg = forward_output['cos_neg']\n",
    "        labels_pos = torch.tensor([1]*pred_pos.shape[0]).to(\"cuda\")\n",
    "        labels_neg = torch.tensor([0]*pred_neg.shape[0]).to(\"cuda\")\n",
    "        \n",
    "        m = self.train_auc(torch.cat((pred_pos,pred_neg),0), torch.cat((labels_pos,labels_neg),0))\n",
    "        \n",
    "        self.log('val_auc', m, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return {\"loss\":loss, 'val_auc':m}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        forward_output = self.forward(batch)\n",
    "        \n",
    "        loss = self.triplet_loss(forward_output['first_embedding'], \n",
    "                                 forward_output['second_embedding'], \n",
    "                                 forward_output['third_embedding'])\n",
    "         \n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return {\"loss\":loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a487efe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trainer(ckpt_dir):\n",
    "\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        mode='max', # wheter we want to maximize (max) or minimize the \"monitor\" value.\n",
    "    )\n",
    "\n",
    "    check_point_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor='val_auc',\n",
    "        verbose=True,\n",
    "        save_top_k=1,\n",
    "        mode='max', # wheter we want to maximize (max) or minimize the \"monitor\" value.\n",
    "        dirpath=ckpt_dir,\n",
    "        filename='{epoch}-{val_auc:.4f}',\n",
    "        # save_weights_only = True\n",
    "    )\n",
    "\n",
    "    # the PyTorch Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=NUM_TRAIN_EPOCHS,\n",
    "        gpus=1,\n",
    "        progress_bar_refresh_rate=5,\n",
    "        # callbacks=[early_stopping, check_point_callback]\n",
    "        callbacks=[check_point_callback]\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b6a7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "data_module = AsmDataModule(train_path, val_path, None, BATCH_SIZE, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446aa519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/olivetree/final_for_paper/models/next_sentence_prediction_bert_normal_mask30/checkpoint-67246\n",
      "From scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(prt_model)\n",
    "simaese_model = SiameseFinenuting(prt_model, BATCH_SIZE, vocab=tokenizer.vocab if from_scratch else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e88d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=5)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = get_trainer(ckpt_dir=output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609cd58",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 39978/39978 [00:11<00:00, 3381.28it/s]\n",
      "100% 9996/9996 [00:02<00:00, 3717.82it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type              | Params\n",
      "---------------------------------------------------\n",
      "0 | model        | BertModel         | 92.0 M\n",
      "1 | triplet_loss | TripletMarginLoss | 0     \n",
      "2 | cosine       | CosineSimilarity  | 0     \n",
      "3 | train_auc    | AUROC             | 0     \n",
      "4 | val_auc      | AUROC             | 0     \n",
      "---------------------------------------------------\n",
      "92.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "92.0 M    Total params\n",
      "368.176   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582929604a4241bd8e4b485c794217b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 4997: val_auc reached 0.96934 (best 0.96934), saving model to \"/home/jovyan/work/olivetree/final_for_paper/tests/similarity/strands/fine_tuned_models/olivetree/from_scratch_normal_triplet_loss/epoch=0-val_auc=0.9693.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 9995: val_auc reached 0.97926 (best 0.97926), saving model to \"/home/jovyan/work/olivetree/final_for_paper/tests/similarity/strands/fine_tuned_models/olivetree/from_scratch_normal_triplet_loss/epoch=1-val_auc=0.9793.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 14993: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 19991: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 24989: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 29987: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 34985: val_auc reached 0.98279 (best 0.98279), saving model to \"/home/jovyan/work/olivetree/final_for_paper/tests/similarity/strands/fine_tuned_models/olivetree/from_scratch_normal_triplet_loss/epoch=6-val_auc=0.9828.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 39983: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 44981: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 49979: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 54977: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 59975: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 64973: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 69971: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 74969: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 79967: val_auc was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 84965: val_auc reached 0.98720 (best 0.98720), saving model to \"/home/jovyan/work/olivetree/final_for_paper/tests/similarity/strands/fine_tuned_models/olivetree/from_scratch_normal_triplet_loss/epoch=16-val_auc=0.9872.ckpt\" as top 1\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=simaese_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##To convert Model Output\n",
    "config = BertConfig(\n",
    "            vocab_size = len(tokenizer.vocab),\n",
    "            max_position_embeddings = MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_size = HIDDEN_SIZE,\n",
    "            intermediate_size = INTERMEDIATE_SIZE,\n",
    "            num_attention_heads = NUM_ATTENTION_HEADS,\n",
    "            num_hidden_layers = NUM_HIDDEN_LAYERS,\n",
    "            type_vocab_size = TYPE_VOCAB_SIZE)\n",
    "model = BertModel(config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_torch = torch.load(os.path.join(output_model_path, \"epoch=16-val_auc=0.9872.ckpt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_dict = dict()\n",
    "for k in model_torch[\"state_dict\"]:\n",
    "    new_dict[k.replace(\"model.\",\"\")] = model_torch[\"state_dict\"][k]\n",
    "del model_torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_state_dict(new_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained(os.path.join(output_model_path, \"epoch-16\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}