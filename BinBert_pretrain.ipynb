{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fastrand\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Transformer tokenizer imports\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "# Transformers data collator\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "# Transformers Bert model\n",
    "from transformers import BertConfig, BertForPreTraining, BertForMaskedLM, Trainer, TrainingArguments, EarlyStoppingCallback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71fa2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and models path settings\n",
    "base_path = \"./\"\n",
    "base_model_path = os.path.join(base_path, \"/models\")\n",
    "base_data_path = os.path.join(base_path, \"/dataset\")\n",
    "\n",
    "\n",
    "train_path = os.path.join(base_data_path, \"train.csv\")\n",
    "val_path = os.path.join(base_data_path, \"val.csv\")\n",
    "\n",
    "TASK_CONFIG = {\"name\": \"next_sentence_prediction\"}\n",
    "\n",
    "tokenizer_path = os.path.join(base_path, \"tokenizer\")\n",
    "\n",
    "# Parameter tokenizer\n",
    "VOCAB_SIZE = 30000\n",
    "MIN_FREQ = 2\n",
    "\n",
    "#Dataset parameters\n",
    "MAX_CHAR_LEN_SYM_EXPR= 5000 #None -- no truncate\n",
    "RECREATE_TOKENIZED_DATASET=True\n",
    "\n",
    "#models\n",
    "BSMAL=\"bert_small\"\n",
    "BNORM=\"bert_normal\"\n",
    "BLARG=\"bert_large\"\n",
    "\n",
    "MODEL=BNORM\n",
    "\n",
    "if MODEL == BSMAL:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 512\n",
    "    INTERMEDIATE_SIZE = 2048\n",
    "    NUM_ATTENTION_HEADS = 8\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "    \n",
    "if MODEL == BNORM:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 768\n",
    "    INTERMEDIATE_SIZE = 3072\n",
    "    NUM_ATTENTION_HEADS = 12\n",
    "    NUM_HIDDEN_LAYERS = 12\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "\n",
    "if MODEL == BLARG:\n",
    "    MAX_SEQ_LEN = 512\n",
    "    MAX_POSITION_EMBEDDINGS = 514\n",
    "    HIDDEN_SIZE = 1024\n",
    "    INTERMEDIATE_SIZE = 4096\n",
    "    NUM_ATTENTION_HEADS = 16\n",
    "    NUM_HIDDEN_LAYERS = 24\n",
    "    TYPE_VOCAB_SIZE = 2\n",
    "\n",
    "model_path = os.path.join(base_model_path, \"{}_{}\".format(TASK_CONFIG[\"name\"],MODEL))\n",
    "\n",
    "# Training Parameters\n",
    "train_from_scratch = True\n",
    "best_ckp = \"\"\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 32\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 64\n",
    "MASKING_RATE = 0.30\n",
    "PATIENCE = 3\n",
    "\n",
    "# GPU settings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"NVIDIA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde66486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(data_path, tokenizer_path, vocab_size, min_freq, max_char_len_sym=None):\n",
    "\n",
    "    \n",
    "    # Prepare data to fed as input to the tokenizer training\n",
    "    df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "    df = df.fillna('')\n",
    "\n",
    "    print(\"Input Data Read\")\n",
    "    if max_char_len_sym==None:\n",
    "        tokenizer_input = df[\"instructions\"].apply(lambda x: x.replace(\"NEXT_I \",\"\")) + \" \" + df[\"sym_expression\"]\n",
    "    else:\n",
    "        tokenizer_input = df[\"instructions\"].apply(lambda x: x.replace(\"NEXT_I \",\"\")) + \" \" + df[\"sym_expression\"].str.slice(0,max_char_len_sym)\n",
    "    \n",
    "        \n",
    "    # Train the tokenizer\n",
    "    tokenizer = BertWordPieceTokenizer()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    tokenizer.train_from_iterator(iterator=tqdm(tokenizer_input,total=len(tokenizer_input)), vocab_size=vocab_size, \n",
    "                                  min_frequency=min_freq)\n",
    "    \n",
    "    # Define the post processing for data\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "      single=\"[CLS] $A [SEP]\",\n",
    "      pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "      special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")), (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\"))]\n",
    "      )\n",
    "    \n",
    "    wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer, model_max_length = MAX_SEQ_LEN)\n",
    "    \n",
    "    wrapped_tokenizer.save_pretrained(tokenizer_path)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80aa532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "  # load tokenizer from dict\n",
    "  tokenizer =  BertTokenizerFast.from_pretrained(tokenizer_path)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b21250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Already Trained!\n"
     ]
    }
   ],
   "source": [
    "# Train Bpe only if it has not been trained yet\n",
    "if not os.path.exists(tokenizer_path):\n",
    "    print(\"Training tokenizer...\")\n",
    "    print(tokenizer_path)\n",
    "    tok = train_tokenizer(train_path, tokenizer_path, VOCAB_SIZE, MIN_FREQ,MAX_CHAR_LEN_SYM_EXPR)\n",
    "    print(\"Training Ended.\")\n",
    "else:\n",
    "    print(\"Tokenizer Already Trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f95298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsmToSymbolicDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, tokenizer, task_config, save_file=None, incorrect_mapping_prob=0.5, num_trials=10, max_char_len_sym=None, batch_size=None):\n",
    "        \n",
    "        self.data_store = []\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.incorrect_mapping_prob = incorrect_mapping_prob\n",
    "        self.num_trials = num_trials\n",
    "        self.max_char_len_sym=max_char_len_sym\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        if save_file == None:\n",
    "            self.df = pd.read_csv(dataset_path, sep=\"\\t\").fillna('')\n",
    "            valid_task_configs = {\"contain_intruders\", \"is_shuffled\", \"next_sentence_prediction\", \"masked_language_model_only\"}\n",
    "            if task_config[\"name\"] not in valid_task_configs:\n",
    "                raise Exception(\"{} is not a valid task_config, please insert one among 'contain_intruders' and 'is_shuffled'\".format(task_config))\n",
    "            else:\n",
    "                self.task_config = task_config\n",
    "            \n",
    "            if task_config[\"name\"] == \"masked_language_model_only\":\n",
    "                self.df = self.df.drop_duplicates([\"instructions\"])\n",
    "\n",
    "            self.__init_structures()\n",
    "        else:\n",
    "            print(\"Loading from save file: {}\".format(save_file))\n",
    "            self.__init_structures_from_file(save_file)\n",
    "        \n",
    "    def __insert_intruders(self, instruction_list, instruction_set):\n",
    "\n",
    "        original_instructions = \" \".join(instruction_list)\n",
    "        modified = False\n",
    "\n",
    "        for i in range(0, self.num_trials):\n",
    "\n",
    "            number_instruders = math.ceil(len(instruction_list) * self.task_config[\"intruder_percentage\"])\n",
    "            positions = random.sample(range(len(instruction_list)),number_instruders)\n",
    "            random_elems =  random.sample(instruction_set, number_instruders)\n",
    "\n",
    "            for position,random_elem in zip(positions,random_elems):\n",
    "                instruction_list[position] = random_elem\n",
    "\n",
    "            modified_instructions = \" \".join(instruction_list)\n",
    "            if modified_instructions != original_instructions:\n",
    "                modified = True\n",
    "                break\n",
    "\n",
    "        return modified, modified_instructions\n",
    "    \n",
    "    def __shuffle(self, instruction_list):\n",
    "\n",
    "        original_instructions = \" \".join(instruction_list)\n",
    "\n",
    "        modified = False\n",
    "\n",
    "        for i in range(0, self.num_trials):\n",
    "            number_instruders = math.ceil(len(instruction_list) * self.task_config[\"shuffling_percentage\"])\n",
    "            positions = random.sample(range(len(instruction_list)), number_instruders)\n",
    "            random_elems =  [elem for i,elem in enumerate(instruction_list) if i in positions]\n",
    "            random.shuffle(random_elems)\n",
    "\n",
    "            for position,random_elem in zip(positions,random_elems):\n",
    "                instruction_list[position] = random_elem\n",
    "            modified_instructions = \" \".join(instruction_list)\n",
    "\n",
    "            if modified_instructions != original_instructions:\n",
    "                modified = True\n",
    "                break\n",
    "                \n",
    "        return modified, modified_instructions\n",
    "   \n",
    "        \n",
    "    def __init_structures(self):\n",
    "        \n",
    "        instruction_set = set(itertools.chain.from_iterable(self.df[\"instructions\"].apply(lambda x: x.split(\"NEXT_I\")).values))\n",
    "        instruction_set = {elem.strip() for elem in instruction_set}\n",
    "        \n",
    "        pairs = self.df[[\"instructions\", \"sym_expression\"]].values\n",
    "        \n",
    "        #accumulators for batch tokenization\n",
    "        accum_x=[]\n",
    "        accum_y=[]\n",
    "        \n",
    "        for x,y in tqdm(pairs):\n",
    "            \n",
    "            if self.task_config[\"name\"] != \"masked_language_model_only\":\n",
    "\n",
    "                is_incorrect = random.random() < self.incorrect_mapping_prob\n",
    "\n",
    "                if is_incorrect:\n",
    "            \n",
    "                    if self.task_config[\"name\"] == \"contain_intruders\":\n",
    "                        instruction_list = x.split(\"NEXT_I\")\n",
    "                        instruction_list = [inst.strip() for inst in instruction_list]\n",
    "                        modified, x = self.__insert_intruders(instruction_list, instruction_set)\n",
    "\n",
    "                    elif self.task_config[\"name\"] == \"is_shuffled\":\n",
    "                        instruction_list = x.split(\"NEXT_I\")\n",
    "                        instruction_list = [inst.strip() for inst in instruction_list]\n",
    "                        modified, x = self.__shuffle(instruction_list)\n",
    "\n",
    "                    elif self.task_config[\"name\"] == \"next_sentence_prediction\":\n",
    "                        modified = False\n",
    "                        for i in range(0, self.num_trials):\n",
    "                            x = x.replace(\"NEXT_I \", \"\")\n",
    "                            #random_idx = random.randint(0, len(pairs) - 1)\n",
    "                            random_idx = fastrand.pcg32bounded(len(pairs))\n",
    "                            y_new = pairs[random_idx][1]\n",
    "                            if y_new != y:\n",
    "                                modified = True\n",
    "                                y = y_new\n",
    "                                break\n",
    "\n",
    "                    label = torch.tensor(1, dtype=torch.long) if modified else torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "                else:\n",
    "                    x = x.replace(\"NEXT_I \", \"\")\n",
    "                    label = torch.tensor(0, dtype=torch.long)\n",
    "                if self.max_char_len_sym != None and len(y)> self.max_char_len_sym:\n",
    "                    y=y[:self.max_char_len_sym]\n",
    "        \n",
    "                if self.batch_size==None:\n",
    "                    example = self.tokenizer(text=x, text_pair=y, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(example[\"input_ids\"], dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(example[\"token_type_ids\"], dtype=torch.long),\n",
    "                        \"next_sentence_label\": label\n",
    "                        }\n",
    "                else:\n",
    "                    accum_x.append(x)\n",
    "                    accum_y.append(y)\n",
    "                    if len(accum_x) == self.batch_size:\n",
    "                        example = self.tokenizer(text=accum_x, text_pair=accum_y, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "                        accum_x.clear()\n",
    "                        accum_y.clear()\n",
    "                    \n",
    "            else:\n",
    "                x = x.replace(\"NEXT_I \", \"\")                \n",
    "                if self.batch_size==None:\n",
    "                    example = self.tokenizer(text=x, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(example[\"input_ids\"], dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(example[\"token_type_ids\"], dtype=torch.long)\n",
    "                    }\n",
    "                else:\n",
    "                    accum_x.append(x)\n",
    "                    if len(accum_x) == self.batch_size:\n",
    "                        example = self.tokenizer(text=x, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "                        accum_x.clear()\n",
    "\n",
    "            self.data_store.append(example)\n",
    "        \n",
    "    def __init_structures_from_file(self,save_file):\n",
    "        self.data_store=torch.load(save_file)\n",
    "                \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_store)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.data_store[idx]\n",
    "    \n",
    "    def save_to_file(self,save_file):\n",
    "        torch.save(self.data_store, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "418c05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b81ce02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or loading train dataset\n",
      "Tokenized train dataset does not appear to exists, creating and saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 2575165/17215046 [14:10<1:01:10, 3989.04it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 25%|██▌       | 4343699/17215046 [23:42<1:10:14, 3054.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_2417357/1094334729.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Tokenized train dataset does not appear to exists, creating and saving\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mtrain_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAsmToSymbolicDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTASK_CONFIG\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmax_char_len_sym\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mMAX_CHAR_LEN_SYM_EXPR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Saving to file..\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;31m#train_dataset.save_to_file(train_tokenized_path)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_2417357/2348030875.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, dataset_path, tokenizer, task_config, save_file, incorrect_mapping_prob, num_trials, max_char_len_sym, batch_size)\u001B[0m\n\u001B[1;32m     22\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop_duplicates\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"instructions\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init_structures\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Loading from save file: {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_2417357/2348030875.py\u001B[0m in \u001B[0;36m__init_structures\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                     \u001B[0mexample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mMAX_SEQ_LEN\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                     example = {\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2513\u001B[0m             )\n\u001B[1;32m   2514\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2515\u001B[0;31m             return self.encode_plus(\n\u001B[0m\u001B[1;32m   2516\u001B[0m                 \u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2517\u001B[0m                 \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mencode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2586\u001B[0m         )\n\u001B[1;32m   2587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2588\u001B[0;31m         return self._encode_plus(\n\u001B[0m\u001B[1;32m   2589\u001B[0m             \u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2590\u001B[0m             \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001B[0m in \u001B[0;36m_encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    520\u001B[0m         \u001B[0;31m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    521\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mreturn_tensors\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mreturn_overflowing_tokens\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 522\u001B[0;31m             batched_output = BatchEncoding(\n\u001B[0m\u001B[1;32m    523\u001B[0m                 {\n\u001B[1;32m    524\u001B[0m                     \u001B[0mkey\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0mn_sequences\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    196\u001B[0m     ):\n\u001B[0;32m--> 197\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mEncodingFast\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/collections/__init__.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    987\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'expected at most 1 arguments, got %d'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    988\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 989\u001B[0;31m             \u001B[0mdict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    990\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0;34m'dict'\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    991\u001B[0m             \u001B[0mdict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'dict'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Creating or loading train dataset\")\n",
    "train_tokenized_path=train_path+\".token\"\n",
    "train_dataset =None\n",
    "if os.path.isfile(train_tokenized_path) and not RECREATE_TOKENIZED_DATASET:\n",
    "    print(\"Tokenized train dataset appear to exists, loading\")\n",
    "    train_dataset = AsmToSymbolicDataset(train_path, tokenizer,TASK_CONFIG, save_file=train_tokenized_path,max_char_len_sym=MAX_CHAR_LEN_SYM_EXPR,batch_size=128)\n",
    "else:\n",
    "    print(\"Tokenized train dataset does not appear to exists, creating and saving\")\n",
    "    train_dataset = AsmToSymbolicDataset(train_path, tokenizer, TASK_CONFIG,max_char_len_sym=MAX_CHAR_LEN_SYM_EXPR)\n",
    "    print(\"Saving to file..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating or loading validation dataset\")\n",
    "val_tokenized_path=val_path+\".token\"\n",
    "val_dataset =None\n",
    "if os.path.isfile(val_tokenized_path) and not RECREATE_TOKENIZED_DATASET:\n",
    "    print(\"Tokenized validation dataset appear to exists, loading\")\n",
    "    val_dataset = AsmToSymbolicDataset(val_path, tokenizer,TASK_CONFIG,  save_file=val_tokenized_path,max_char_len_sym=MAX_CHAR_LEN_SYM_EXPR)\n",
    "else:\n",
    "    print(\"Tokenized validation dataset does not appear to exists, creating and saving\")\n",
    "    val_dataset = AsmToSymbolicDataset(val_path, tokenizer, TASK_CONFIG,max_char_len_sym=MAX_CHAR_LEN_SYM_EXPR)\n",
    "    print(\"Saving to file..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9beae14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=MASKING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7599893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8605970 Incorrent samples over 17215046 total samples\n"
     ]
    }
   ],
   "source": [
    "if TASK_CONFIG[\"name\"] != \"masked_language_model_only\":\n",
    "    total_samples = len(train_dataset.data_store)\n",
    "    incorrect_samples = sum([1 for elem in train_dataset.data_store if elem [\"next_sentence_label\"] == 1 ])\n",
    "    print(f\"{incorrect_samples} Incorrent samples over {total_samples} total samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50fc6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confignew = BertConfig(\n",
    "                vocab_size = len(tokenizer.vocab),\n",
    "                max_position_embeddings = MAX_POSITION_EMBEDDINGS,\n",
    "                hidden_size = HIDDEN_SIZE,\n",
    "                intermediate_size = INTERMEDIATE_SIZE,\n",
    "                num_attention_heads = NUM_ATTENTION_HEADS,\n",
    "                num_hidden_layers = NUM_HIDDEN_LAYERS,\n",
    "                type_vocab_size = TYPE_VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2cbf57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  92645512\n"
     ]
    }
   ],
   "source": [
    "if TASK_CONFIG[\"name\"] == \"masked_language_model_only\":\n",
    "    model = BertForMaskedLM(config=confignew)\n",
    "else:\n",
    "    model = BertForPreTraining(config=confignew)\n",
    "    \n",
    "model.train()\n",
    "\n",
    "print(\"Total number of parameters: \", model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26d5fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "            output_dir = model_path,\n",
    "            overwrite_output_dir = True,\n",
    "            num_train_epochs = NUM_TRAIN_EPOCHS,\n",
    "            learning_rate = LEARNING_RATE,\n",
    "            per_device_train_batch_size = PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "            save_strategy = 'epoch',\n",
    "            # save_steps=7126,\n",
    "            # save_steps=67246,\n",
    "            save_total_limit = 100,\n",
    "            warmup_steps =1425,\n",
    "            logging_strategy = 'steps',\n",
    "            logging_steps = 500,\n",
    "            prediction_loss_only = True,\n",
    "            load_best_model_at_end = True,\n",
    "            # fp16=True,\n",
    "            do_eval = True,\n",
    "            gradient_accumulation_steps=2,\n",
    "            evaluation_strategy = 'epoch',\n",
    "            # eval_steps=7126,\n",
    "            # eval_steps=67246\n",
    "            metric_for_best_model = 'eval_loss',\n",
    "            per_device_eval_batch_size = 128,\n",
    "            dataloader_num_workers =4,\n",
    "            dataloader_pin_memory=True\n",
    "            )\n",
    "\n",
    "trainer = Trainer(\n",
    "            model = model,\n",
    "            args = training_args,\n",
    "            data_collator = mlm_collator,\n",
    "            train_dataset = train_dataset,\n",
    "            eval_dataset = val_dataset,\n",
    "            # callbacks = [EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa9be799",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d9267",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17215046\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 67246\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62170' max='67246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62170/67246 11:26:01 < 56:00, 1.51 it/s, Epoch 0.92/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_from_scratch:\n",
    "    trainer.train()\n",
    "else:\n",
    "    trainer.train(best_ckp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}